{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this exercise we explore $K$-means clustering - and we it out on the locations of the `PROSTITUTION` crime type. Applying a clustering method makes sense because we know from our earlier work that this crime type tends to happen in only a few locations. We'll also talk a little bit about model selection and [overfitting](https://www.youtube.com/watch?v=DQWI1kvmwRg) in unsupervised models.\n",
    "\n",
    "> _Exercise_: $K$-means\n",
    "> \n",
    "> * Visualize the prostitution data (e.g. by plotting it on a map)\n",
    "> * Train models of $K = 2,\\ldots,10$ on the prostitution data.\n",
    "> * Explore how the total squared error changes as a function of $K$ and identify what you think is the right number of clusers based on the knee-point in the squared error plot.\n",
    "> * And by the way: The fit only gets better when we add more means - why not keep adding more of them: Explain in your own words why it makes sense to stop around a knee-point.\n",
    "> * Another way of estimating the right number of clusters in a $K$-means problem is _stability analysis_. The idea is the following\n",
    ">   - For each $K = 2,\\ldots,10$ generate $N = 10$ clusterings based on random 50% of data (or some other fraction of data/bootstrap).\n",
    ">   - Calculate the pairwise similarity between the clusterings. \n",
    ">   - We now define _stability_ for some value of $K$ as average pairwise similarity of the $N$ clustering, where the similarity is the cosine distance $\\frac{\\mathbf{c}_i^K\\cdot\\mathbf{c}_j^K}{||\\mathbf{c}_i^K||\\,||\\mathbf{c}_j^K||}$ between centroid vectors $\\mathbf{c}_i^K$ and $\\mathbf{c}_j^K$.\n",
    ">   - We now say that the right $K$ maximizes stability.\n",
    "> * Explain why stability should help you find the right number of clusters.\n",
    "> * **Optional**: Perform stability analysis on the prostitution data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import collections\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import geoplotlib\n",
    "from geoplotlib.utils import BoundingBox, DataAccessObject\n",
    "\n",
    "data = open(\"SFPD_Incidents_-_from_1_January_2003.csv\")\n",
    "next(data,1).split(',')\n",
    "\n",
    "category = []\n",
    "description = []\n",
    "day_of_week = []\n",
    "date = []\n",
    "time = []\n",
    "police_district = []\n",
    "lon = []\n",
    "lat = []\n",
    "\n",
    "for line in reader(data): \n",
    "    var1, var2, var3, var4, var5, var6, var7, var8, var9, var10, var11, var12, var13 = line\n",
    "    \n",
    "    category.append(var2)\n",
    "    description.append(var3)\n",
    "    day_of_week.append(var4)\n",
    "    date.append(var5)\n",
    "    time.append(var6)\n",
    "    police_district.append(var7)\n",
    "    lon.append(float(var10))\n",
    "    lat.append(float(var11))\n",
    "\n",
    "#Bounding box\n",
    "eps_center = 0.05\n",
    "max_lat = np.mean(lat)+eps_center\n",
    "min_lat = np.mean(lat)-eps_center\n",
    "max_lon = np.mean(lon)+eps_center\n",
    "min_lon = np.mean(lon)-eps_center\n",
    "bbox = BoundingBox(north=max_lat, west=min_lon, south=min_lat, east=max_lon)\n",
    "\n",
    "lat = np.asarray(lat).astype(np.float)\n",
    "lon = np.asarray(lon).astype(np.float)\n",
    "\n",
    "focuscrimes = ['PROSTITUTION', 'DRUG/NARCOTIC', 'DRIVING UNDER THE INFLUENCE']\n",
    "\n",
    "\n",
    "# Find incidents in the district\n",
    "for crime in focuscrimes:\n",
    "    mask = np.zeros_like(category,dtype='bool')\n",
    "    for i in range(len(category)):\n",
    "        if category[i] == 'PROSTITUTION':\n",
    "            mask[i] = True\n",
    "    data = {'lat': lat[mask], 'lon': lon[mask]}\n",
    "    \n",
    "geoplotlib.dot(data)\n",
    "geoplotlib.set_bbox(bbox)\n",
    "geoplotlib.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The code above is a visulisation of the prostitution data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from geoplotlib.colors import create_set_cmap\n",
    "import pyglet\n",
    "from sklearn.cluster import KMeans\n",
    "import geoplotlib\n",
    "from geoplotlib.layers import BaseLayer\n",
    "from geoplotlib.core import BatchPainter\n",
    "from geoplotlib.utils import BoundingBox\n",
    "import numpy as np\n",
    "\n",
    "class KMeansLayer(BaseLayer):\n",
    "    \n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.k = 2\n",
    "\n",
    "\n",
    "    def invalidate(self, proj):\n",
    "        self.painter = BatchPainter()\n",
    "        x, y = proj.lonlat_to_screen(self.data['lon'], self.data['lat'])\n",
    "\n",
    "        k_means = KMeans(n_clusters=self.k)\n",
    "        k_means.fit(np.vstack([x,y]).T)\n",
    "        labels = k_means.labels_\n",
    "\n",
    "        self.cmap = create_set_cmap(set(labels), 'hsv')\n",
    "        for l in set(labels):\n",
    "            self.painter.set_color(self.cmap[l])\n",
    "            #Enabling the line below enables the convexhull algorithm.\n",
    "            #self.painter.convexhull(x[labels == l], y[labels == l])\n",
    "            self.painter.points(x[labels == l], y[labels == l], 5)\n",
    "    \n",
    "            \n",
    "    def draw(self, proj, mouse_x, mouse_y, ui_manager):\n",
    "        ui_manager.info('Use left and right to increase/decrease the number of clusters. k = %d' % self.k)\n",
    "        self.painter.batch_draw()\n",
    "\n",
    "\n",
    "    def on_key_release(self, key, modifiers):\n",
    "        if key == pyglet.window.key.LEFT:\n",
    "            self.k = max(2,self.k - 1)\n",
    "            return True\n",
    "        elif key == pyglet.window.key.RIGHT:\n",
    "            self.k = self.k + 1\n",
    "            return True\n",
    "        return False\n",
    "  \n",
    "\n",
    "geoplotlib.add_layer(KMeansLayer(data))\n",
    "geoplotlib.set_smoothing(True)\n",
    "geoplotlib.set_bbox(bbox)\n",
    "geoplotlib.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Above is the Train model from 2 to 10 (or above), since K in this program can be increased by using the arrorw keys.\n",
    "\n",
    "* I belive the right number of clusters is found by starting with one cluster (k=1) and then keep dividing clusters until the points assigned to each cluster have a Gaussian distribution.\n",
    "\n",
    "* The reason why we are not interested in continously adding more means is because if you keep splitting to 'infinitity', you'll just end up with a cluster around every single item in your data set.\n",
    "\n",
    "* Stability analysis should help us find the right number of cluters due to cosine distance comparison of clusters. We know that the if cosine distance is equal to 1 then the similarity between clusters has reached its maximum stability. It should be mentioned that reaching 1 is not nessecarily possible, but as close to 1 as possible will surfice. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
